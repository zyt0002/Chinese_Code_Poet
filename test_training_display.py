#!/usr/bin/env python3

# æµ‹è¯•è®­ç»ƒæ˜¾ç¤ºé…ç½®
from trl import SFTConfig
from transformers import TrainerCallback

print("=== æµ‹è¯•è®­ç»ƒæ˜¾ç¤ºé…ç½® ===")

# æµ‹è¯•SFTConfigå‚æ•°
training_args = SFTConfig(
    output_dir="./test_output",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=15,
    save_strategy="epoch",
    save_steps=500,
    logging_steps=1,
    logging_strategy="steps",
    eval_strategy="no",
    report_to="tensorboard",
    logging_first_step=True,
    optim="adamw_torch",
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    bf16=True,
    dataloader_drop_last=True,
    remove_unused_columns=False,
    disable_tqdm=False,
    log_level="info",
    logging_nan_inf_filter=True,
)

print("âœ… SFTConfig åˆ›å»ºæˆåŠŸ")
print(f"âœ… æ—¥å¿—æ­¥æ•°: {training_args.logging_steps}")
print(f"âœ… è¿›åº¦æ¡å¯ç”¨: {not training_args.disable_tqdm}")
print(f"âœ… æ—¥å¿—çº§åˆ«: {training_args.log_level}")

# æµ‹è¯•å›è°ƒå‡½æ•°
class ProgressCallback(TrainerCallback):
    def on_epoch_begin(self, args, state, control, **kwargs):
        print(f"\nğŸš€ å¼€å§‹ Epoch {int(state.epoch) + 1}/{args.num_train_epochs}")
        
    def on_epoch_end(self, args, state, control, **kwargs):
        print(f"âœ… å®Œæˆ Epoch {int(state.epoch)}/{args.num_train_epochs}")
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and "loss" in logs:
            current_step = state.global_step
            total_steps = state.max_steps if state.max_steps > 0 else "Unknown"
            current_epoch = state.epoch
            print(f"Step {current_step}/{total_steps} | Epoch {current_epoch:.2f} | Loss: {logs['loss']:.4f}")

callback = ProgressCallback()
print("âœ… è¿›åº¦å›è°ƒå‡½æ•°åˆ›å»ºæˆåŠŸ")

# æ¨¡æ‹Ÿè®­ç»ƒä¿¡æ¯è®¡ç®—
dataset_size = 285
batch_size = training_args.per_device_train_batch_size
grad_accum = training_args.gradient_accumulation_steps
epochs = training_args.num_train_epochs

effective_batch_size = batch_size * grad_accum
steps_per_epoch = dataset_size // effective_batch_size
total_steps = steps_per_epoch * epochs

print(f"\n=== è®­ç»ƒé…ç½®ä¿¡æ¯ ===")
print(f"æ•°æ®é›†å¤§å°: {dataset_size}")
print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {grad_accum}")
print(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
print(f"æ¯è½®æ­¥æ•°: {steps_per_epoch}")
print(f"æ€»è½®æ•°: {epochs}")
print(f"é¢„è®¡æ€»æ­¥æ•°: {total_steps}")

print("\nâœ… è®­ç»ƒæ˜¾ç¤ºé…ç½®æµ‹è¯•å®Œæˆï¼ç°åœ¨åº”è¯¥èƒ½çœ‹åˆ°è¯¦ç»†çš„epochå’Œæ­¥æ•°ä¿¡æ¯ã€‚")